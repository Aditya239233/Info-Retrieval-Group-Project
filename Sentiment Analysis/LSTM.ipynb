{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Download fastai library\n",
    "! pip install fastbook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from fastbook import *\n",
    "from IPython.display import display,HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Import NLP module from fastai library\n",
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Read 1/6th of the train dataset to reduce total time taken for training\n",
    "train = pd.read_csv('train_unbalanced_final.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Read 1/6th of the test dataset to reduce total time taken for testing\n",
    "test = pd.read_csv('test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Check major statistics for the train dataset\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Check major statistics for the test dataset\n",
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Check the distribution of the number of data points for each of the 5 classes\n",
    "train['classes'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# The distribution seems fairly equal which is perfect for training the model\n",
    "train['classes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Array of all the reviews in the train data frame\n",
    "txts = L([i for i in train['content']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Tokenize all the rows\n",
    "tok = Tokenizer.from_df(train)\n",
    "tok.setup(train)\n",
    "\n",
    "toks = txts.map(tok)\n",
    "toks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Numericalize all the tokens from the previous step\n",
    "num = Numericalize()\n",
    "num.setup(toks)\n",
    "nums = toks.map(num)\n",
    "nums[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Seeing the indexes of how the words are stored for the first row only\n",
    "num.encodes(toks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Decoding the indexes to see the tokens\n",
    "num.decode(nums[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Concatening just the reviews column of both train and test datasets to help in creating the language model\n",
    "language_model = pd.concat([train, test], axis=0)[['content']]\n",
    "language_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Checking major statistics of the new dataset\n",
    "language_model.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Splitting the new dataset randomly into 90% train and 10% validation \n",
    "data_lm = DataBlock(\n",
    "    blocks=TextBlock.from_df('content', is_lm=True),\n",
    "    get_x=ColReader('text'), \n",
    "    splitter=RandomSplitter(0.1) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Internally tokenizing and numericalizing the data. The sequence length used is the default used for training the Wikipedia 103 language model\n",
    "data_lm = data_lm.dataloaders(language_model, bs=64, seq_len=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Randomly show 5 of the rows from the language model\n",
    "data_lm.show_batch(max_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Defining the LSTM architecture of the train model and the metrics used to acertain its accuracy\n",
    "learn_model = language_model_learner(\n",
    "    data_lm, AWD_LSTM, drop_mult=0.3,\n",
    "    metrics=[accuracy, Perplexity()]).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Check the various layers of the model\n",
    "learn_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Find the best learning rate for training the language model\n",
    "learn_model.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Fine tuning the language model based on the datablock which would enable in predicting the next word in a sentence for the Amazon Reviews dataset specifically\n",
    "learn_model.fine_tune(4, 1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Save the encoder that understands the intricate language semantics of our dataset\n",
    "learn_model.save_encoder('finetuned_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Load the encoder that understands the intricate language semantics of our dataset\n",
    "learn_model.load_encoder('finetuned_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Predict the full sentence of a negative prompt using our trained language model\n",
    "learn_model.predict(\"This laptop was horrible because\", 30, temperature=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Predict the full sentence of a positive prompt using our trained language model\n",
    "learn_model.predict(\"I absolutely loved this dress because\", 20, temperature=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Download necessary libraries for Easy Data Augmentation (EDA)\n",
    "!pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Download necessary libraries for Easy Data Augmentation (EDA)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Make new dataframe containing 10x less data than the train dataset because EDA would generate new data from lesser existing data\n",
    "eda = pd.read_csv('train.csv')\n",
    "eda = eda[[\"classes\", \"content\"]]\n",
    "eda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Convert csv to txt file required for EDA to run\n",
    "eda.to_csv('train2.txt', header=False, index=False, sep='\\t', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Run the python command for EDA with 10 new augmentations per existing tweet and perform all 4 augmentations with its default alpha values as stated in the research paper\n",
    "%run eda_nlp/code/augment.py --input=eda_nlp/data/train2.txt --num_aug=10 --alpha_sr=0.05 --alpha_rd=0.00 --alpha_ri=0.05 --alpha_rs=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Convert txt to csv and replace train dataset with new EDA data\n",
    "train = pd.read_csv('eda_train2.txt', delimiter=\"\\t\", header=None, names=[\"classes\", \"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train[[\"classes\", \"content\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Check data to see if each data point is repeated 3 times, first is original and the next 2 are augmentations\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the datablock for the classifier now splitting the dataset into 80% train and 20% validation\n",
    "blocks = (TextBlock.from_df('content', seq_len=data_lm.seq_len, vocab=data_lm.vocab), CategoryBlock())\n",
    "data_classifier = DataBlock(blocks=blocks,\n",
    "                get_x=ColReader('text'),\n",
    "                get_y=ColReader('classes'),\n",
    "                splitter=RandomSplitter(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Perform tokenization and numericalization of the loaded data automatically\n",
    "data_classifier = data_classifier.dataloaders(train, bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Randomly show 5 of the rows from the classifier\n",
    "data_classifier.show_batch(max_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Defining the LSTM architecture of the train model and the metrics used to acertain its accuracy\n",
    "learn_model = text_classifier_learner(data_classifier, AWD_LSTM, metrics=[accuracy, F1Score(average=\"micro\")], drop_mult=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load up the previously trained language model encoder\n",
    "learn_model.load_encoder('finetuned_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Find the best learning rate for training the classifer\n",
    "learn_model.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Train the model one epoch with the min learning rate from previous step\n",
    "learn_model.fit_one_cycle(1, 2e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Freeze all the layers in the model except for the last two and train again with a smaller learning rate, the sliced values follows the instructions as per Jeremy Howard's FastAI course\n",
    "learn_model.freeze_to(-2)\n",
    "learn_model.fit_one_cycle(1, slice(2e-2/(2.6**4), 2e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Freeze all the layers in the model except for the last three and train again with a smaller learning rate, the sliced values follows the instructions as per Jeremy Howard's FastAI course\n",
    "learn_model.freeze_to(-3)\n",
    "learn_model.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Unfreeze all the layers in the model and train again with a smaller learning rate, the sliced values follows the instructions as per Jeremy Howard's FastAI course\n",
    "learn_model.unfreeze()\n",
    "learn_model.fit_one_cycle(5, slice(1e-3/(2.6**4),1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Save the final trained model\n",
    "learn_model.save('trained_model_eda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Export the final trained model\n",
    "learn_model.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the final trained model\n",
    "learn_model.load('trained_model_eda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test['one_hot_labels'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_model.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(5, nrows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "test['preds'] = test['content'].apply(lambda row: learn_model.predict(row)[0])\n",
    "print(\"Test Accuracy: \", accuracy_score(test['classes'], test['preds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"preds3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = learn_model.dls.test_dl(test['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = learn_model.get_preds(dl=dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0][0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0][0].cpu().argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['target'] = preds[0].argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"preds4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}